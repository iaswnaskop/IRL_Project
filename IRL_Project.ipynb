{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0fLRYc8grPh"
      },
      "source": [
        "# Εργασία Ανάκτησης Πληροφορίας\n",
        "Δημιουργία μηχανής αναζήτησης"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPwai02Egsvk"
      },
      "source": [
        "## Εισαγωγή βιβλιοθηκών"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0Hj6SR7gukb",
        "outputId": "4348bf8b-ffad-4766-c97e-c6bb0b7cc3e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.3.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.10)\n",
            "Requirement already satisfied: rank-bm25 in /usr/local/lib/python3.10/dist-packages (0.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rank-bm25) (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "! pip install kaggle\n",
        "! pip install rank-bm25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dstGAE98gvI6",
        "outputId": "6db1a443-5fdc-44ee-8111-6c9b82ae459d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import nltk\n",
        "import re\n",
        "import json\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score\n",
        "\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-0VNQ9JCaDg"
      },
      "source": [
        "## Βήμα 1. Συλλογή δεδομένων"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nx07SCYZCeC3",
        "outputId": "c036af10-d80d-41d7-d2c0-429dac294b22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/jrobischon/wikipedia-movie-plots\n",
            "License(s): CC-BY-SA-4.0\n",
            "Downloading wikipedia-movie-plots.zip to /content\n",
            "100% 29.9M/29.9M [00:02<00:00, 21.0MB/s]\n",
            "100% 29.9M/29.9M [00:02<00:00, 12.5MB/s]\n",
            "Archive:  /content/wikipedia-movie-plots.zip\n",
            "  inflating: wiki_movie_plots_deduped.csv  \n"
          ]
        }
      ],
      "source": [
        "# Εισαγωγή του API Key και του ονόματος χρήστη από το Kaggle (μέσω kaggle.json)\n",
        "os.environ[\"KAGGLE_KEY\"] = 'f8d5210b1add9082c88d61aed904ccab'\n",
        "os.environ[\"KAGGLE_USERNAME\"] = 'iaswnas'\n",
        "\n",
        "# Λήψη του dataset από το Kaggle\n",
        "!kaggle datasets download jrobischon/wikipedia-movie-plots\n",
        "\n",
        "# Αποσυμπίεση του dataset\n",
        "!unzip /content/wikipedia-movie-plots.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyLsBEsqjcia",
        "outputId": "d25fc6cc-3c0e-4bdc-e189-e6c37dd6e2f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessed data saved to preprocessed_plots.csv\n"
          ]
        }
      ],
      "source": [
        "def preprocess_movie_plots(csv_filepath, output_filepath=\"preprocessed_plots.csv\"):\n",
        "    \"\"\"Reads the original movie CSV, prepends titles to plots, saves combined text to a new CSV file.\"\"\"\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(csv_filepath)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {csv_filepath}\")\n",
        "        return\n",
        "\n",
        "    if not all(col in df.columns for col in [\"Title\", \"Plot\"]):\n",
        "        print(\"Error: CSV must have 'Title' and 'Plot' columns.\")\n",
        "        return\n",
        "\n",
        "    df[\"Combined\"] = df[\"Title\"] + \": \" + df[\"Plot\"]\n",
        "\n",
        "    # Keep only the \"Combined\" column\n",
        "    df = df[[\"Combined\"]]\n",
        "\n",
        "    try:\n",
        "        df.to_csv(output_filepath, index=False, header=False, encoding=\"utf-8\")\n",
        "        print(f\"Preprocessed data saved to {output_filepath}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving CSV: {e}\")\n",
        "\n",
        "csv_filepath = \"/content/wiki_movie_plots_deduped.csv\"\n",
        "preprocess_movie_plots(csv_filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkD4OEIewdZY",
        "outputId": "75e21952-27bb-40c1-90a8-d8141525a73a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reduced CSV saved to /content/reduced_movie_plots.csv\n"
          ]
        }
      ],
      "source": [
        "def reduce_csv_size(input_filepath, output_filepath, fraction=0.01):\n",
        "    \"\"\"Reduces the size of a CSV file by keeping a specified fraction of the rows.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(input_filepath)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {input_filepath}\")\n",
        "        return\n",
        "\n",
        "    reduced_df = df.sample(frac=fraction, random_state=42)  # a specific random state for reproducibility of results\n",
        "\n",
        "\n",
        "    reduced_df.to_csv(output_filepath, index=False)\n",
        "    print(f\"Reduced CSV saved to {output_filepath}\")\n",
        "\n",
        "# Example usage in Colab:\n",
        "input_csv = \"/content/preprocessed_plots.csv\"  # preprocessed CSV location\n",
        "output_csv = \"/content/reduced_movie_plots.csv\" # New truncated CSV location\n",
        "reduce_csv_size(input_csv, output_csv, fraction=0.01) # Keep a fraction of the original data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atHQnfwmCJUX"
      },
      "source": [
        "## Βήμα 2. Προεπεξεργασία κειμένου (Text Processing):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWDshJsckq2x",
        "outputId": "06d4fae8-aa54-4724-90c8-fbd6667f1a72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessed data saved to preprocessed_data.csv\n"
          ]
        }
      ],
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"Preprocesses a single text string.\"\"\"\n",
        "\n",
        "    # 1. Lowercasing\n",
        "    text = text.lower()\n",
        "\n",
        "    # 2. Remove special characters and numbers\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Keep only letters and spaces\n",
        "\n",
        "    # 3. Tokenization\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # 4. Stop word removal\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # 5. Stemming (or Lemmatization - uncomment the relevant part)\n",
        "    stemmer = PorterStemmer()\n",
        "    # lemmatizer = WordNetLemmatizer()  # lemmatization\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "    # lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens] # lemmatization\n",
        "\n",
        "    # 6. Join tokens back into a string\n",
        "    processed_text = \" \".join(stemmed_tokens)  # Or lemmatized_tokens\n",
        "\n",
        "    return processed_text\n",
        "\n",
        "def preprocess_csv(csv_filepath, output_filepath=\"preprocessed_data.csv\"):\n",
        "    \"\"\"Preprocesses the text data in a CSV file.\"\"\"\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(csv_filepath, header=None, names=[\"text\"])  # Assumes no header\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {csv_filepath}\")\n",
        "        return\n",
        "\n",
        "\n",
        "    df['processed_text'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "    df.to_csv(output_filepath, index=False, encoding=\"utf-8\")  # Save to new CSV\n",
        "    print(f\"Preprocessed data saved to {output_filepath}\")\n",
        "\n",
        "# Pre-process dataset:\n",
        "csv_filepath = \"/content/reduced_movie_plots.csv\"\n",
        "preprocess_csv(csv_filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXUwywoNCPcx"
      },
      "source": [
        "## Βήμα 3. Ευρετήριο (Indexing):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KkVitK9q2ZC",
        "outputId": "81f5f55d-c29d-4aef-a36a-617cea704d88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inverted index saved to inverted_index.json\n"
          ]
        }
      ],
      "source": [
        "def create_inverted_index(csv_filepath, output_filepath=\"inverted_index.json\"):\n",
        "    \"\"\"Creates an inverted index from the preprocessed CSV.\"\"\"\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(csv_filepath, header=None, names=[\"text\"])\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {csv_filepath}\")\n",
        "        return\n",
        "\n",
        "    inverted_index = {}\n",
        "    for doc_id, row in df.iterrows():  # Use doc_id as document identifier\n",
        "        text = row['text']\n",
        "        terms = text.split()  # Split terms\n",
        "\n",
        "        for term in terms:\n",
        "            if term not in inverted_index:\n",
        "                inverted_index[term] = []  # Initialize postings list for the term\n",
        "            inverted_index[term].append(doc_id)\n",
        "\n",
        "\n",
        "    # Save the inverted index to a JSON file:\n",
        "    with open(output_filepath, 'w', encoding='utf-8') as f:\n",
        "        json.dump(inverted_index, f, indent=4)\n",
        "\n",
        "    print(f\"Inverted index saved to {output_filepath}\")\n",
        "\n",
        "# Finally, create the inverted index:\n",
        "csv_filepath = \"/content/reduced_movie_plots.csv\"\n",
        "create_inverted_index(csv_filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJ4KPMF3CSKo"
      },
      "source": [
        "## Βήμα 4. Μηχανή αναζήτησης (Search Engine)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "VcvUqMrarmpC"
      },
      "outputs": [],
      "source": [
        "def load_inverted_index(filepath):\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def load_documents(filepath):\n",
        "    df = pd.read_csv(filepath, header=None, names=[\"text\"])\n",
        "    return df['text'].tolist()\n",
        "\n",
        "def boolean_search(query, inverted_index, documents):\n",
        "    query_terms = [preprocess_text(term) for term in query.split()]\n",
        "    result_docs = set(range(len(documents))) # Start with all documents\n",
        "\n",
        "    for term in query_terms:\n",
        "        if term.startswith(\"-\"):  # NOT operator\n",
        "            term = term[1:]\n",
        "            try:\n",
        "                result_docs -= set(inverted_index[term])\n",
        "            except KeyError:\n",
        "                pass  # Term not in index, ignore\n",
        "        elif \" or \" in term.lower(): # OR operator\n",
        "            sub_terms = [t.strip() for t in term.split(\" or \")]\n",
        "            or_docs = set()\n",
        "            for sub_term in sub_terms:\n",
        "                try:\n",
        "                    or_docs.update(inverted_index[sub_term])\n",
        "                except KeyError:\n",
        "                    pass\n",
        "            result_docs.intersection_update(or_docs)\n",
        "\n",
        "\n",
        "        else:  # AND (default)\n",
        "            try:\n",
        "                result_docs.intersection_update(inverted_index[term])\n",
        "            except KeyError:\n",
        "                result_docs = set()  # Term not found, empty results\n",
        "                break\n",
        "\n",
        "    return [documents[doc_id] for doc_id in sorted(result_docs)]\n",
        "\n",
        "def tfidf_search(query, documents):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "    query_vec = vectorizer.transform([query])\n",
        "    scores = (query_vec * tfidf_matrix.T).toarray()\n",
        "    ranked_docs_indices = scores.argsort()[0][::-1]  # Sort descending\n",
        "    return [documents[i] for i in ranked_docs_indices]\n",
        "\n",
        "def bm25_search(query, documents):\n",
        "    tokenized_corpus = [doc.split(\" \") for doc in documents]\n",
        "    bm25 = BM25Okapi(tokenized_corpus)\n",
        "    tokenized_query = query.split(\" \")\n",
        "    doc_scores = bm25.get_scores(tokenized_query)\n",
        "    ranked_docs_indices = doc_scores.argsort()[::-1]\n",
        "    return [documents[i] for i in ranked_docs_indices]\n",
        "\n",
        "def search_engine(query, inverted_index, documents, search_type=\"boolean\"):\n",
        "\n",
        "    if search_type == \"tfidf\":\n",
        "        results = tfidf_search(query, documents)\n",
        "    elif search_type == \"bm25\":\n",
        "        results = bm25_search(query, documents)\n",
        "    else:  # Default to boolean\n",
        "        results = boolean_search(query, inverted_index, documents)\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxyCmB-5udgJ",
        "outputId": "c2c0543a-6ee7-49be-9415-52ae7d8b4e93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your search query: war OR prison\n",
            "Enter search type (boolean, tfidf, bm25): boolean\n",
            "No results found.\n"
          ]
        }
      ],
      "source": [
        "# Use our search engine\n",
        "inverted_index_filepath = \"/content/inverted_index.json\"\n",
        "documents_filepath = \"/content/reduced_movie_plots.csv\"\n",
        "\n",
        "inverted_index = load_inverted_index(inverted_index_filepath)\n",
        "documents = load_documents(documents_filepath)\n",
        "\n",
        "query = input(\"Enter your search query: \")\n",
        "search_type = input(\"Enter search type (boolean, tfidf, bm25): \") # User enters the query type to perform\n",
        "\n",
        "results = search_engine(query, inverted_index, documents, search_type)\n",
        "\n",
        "if results:\n",
        "    print(\"\\nSearch Results:\")\n",
        "    for i, result in enumerate(results[:20]): # Show first 20 documents\n",
        "        print(f\"{i+1}. {result[:150]}...\") # Show first 150 characters of each result\n",
        "        print(\"-\" * 20)\n",
        "else:\n",
        "    print(\"No results found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIcYrilXCULf"
      },
      "source": [
        "## Βήμα 5. Αξιολόγηση συστήματος"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaRCgZClz_0C",
        "outputId": "8e3a9927-ac49-46f8-d3cf-3f2e2681037e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessed data with doc_id saved to preprocessed_data_with_id.csv\n",
            "Mean precision: 0.5263\n",
            "Mean recall: 0.5263\n",
            "Mean f1: 0.5263\n"
          ]
        }
      ],
      "source": [
        "def preprocess_csv_with_doc_id(input_filepath, output_filepath=\"preprocessed_data_with_id.csv\"):\n",
        "    try:\n",
        "        df = pd.read_csv(input_filepath, header=None, names=[\"text\"])  # Or specify correct header if present\n",
        "    except pd.errors.EmptyDataError:  # Handle empty CSV\n",
        "        print(f\"Error: CSV file '{input_filepath}' is empty or contains only headers.\")\n",
        "        return\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{input_filepath}' not found.\")\n",
        "        return\n",
        "    except pd.errors.ParserError as e:  # Handle parsing errors\n",
        "        print(f\"Error parsing CSV: {e}\")\n",
        "        return\n",
        "\n",
        "    df['processed_text'] = df['text'].apply(preprocess_text)\n",
        "    df['doc_id'] = range(len(df))\n",
        "    df = df[['doc_id', 'processed_text']]\n",
        "    df.to_csv(output_filepath, index=False, encoding=\"utf-8\")\n",
        "    print(f\"Preprocessed data with doc_id saved to {output_filepath}\")\n",
        "\n",
        "def boolean_evaluation(queries, df):\n",
        "    metrics = defaultdict(list)\n",
        "\n",
        "    for query_text, keywords in queries.items():\n",
        "\n",
        "        # Simulated relevance judgments:\n",
        "        relevant_df = df[df['processed_text'].str.contains('|'.join(keywords), na=False, regex=True)]\n",
        "        relevant_doc_ids = relevant_df['doc_id'].tolist()\n",
        "\n",
        "        # Boolean search\n",
        "        retrieved_df = df[df['processed_text'].str.contains('|'.join(keywords), na=False, regex=True)]\n",
        "        retrieved_doc_ids = retrieved_df['doc_id'].tolist()\n",
        "\n",
        "\n",
        "        all_doc_ids = set(df['doc_id'].tolist())\n",
        "        relevant_docs_binary = [1 if doc_id in relevant_doc_ids else 0 for doc_id in all_doc_ids]\n",
        "        retrieved_docs_binary = [1 if doc_id in retrieved_doc_ids else 0 for doc_id in all_doc_ids]\n",
        "\n",
        "\n",
        "\n",
        "        if any(retrieved_docs_binary):  # Only calculate if documents were retrieved\n",
        "            precision = precision_score(relevant_docs_binary, retrieved_docs_binary)\n",
        "            recall = recall_score(relevant_docs_binary, retrieved_docs_binary)\n",
        "            f1 = f1_score(relevant_docs_binary, retrieved_docs_binary)\n",
        "        else:\n",
        "            precision, recall, f1 = 0, 0, 0\n",
        "\n",
        "\n",
        "        metrics[\"precision\"].append(precision)\n",
        "        metrics[\"recall\"].append(recall)\n",
        "        metrics[\"f1\"].append(f1)\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Perform evaluation\n",
        "input_filepath = \"/content/reduced_movie_plots.csv\"\n",
        "preprocess_csv_with_doc_id(input_filepath)\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"preprocessed_data_with_id.csv\")\n",
        "\n",
        "queries = {\n",
        "    \"love\": [\"love\"],\n",
        "    \"war\": [\"war\"],\n",
        "    \"friend\": [\"friend\"],\n",
        "    \"family\": [\"family\"],\n",
        "    \"space\": [\"space\"],\n",
        "    \"romantic comedy\": [\"romantic\", \"comedy\"],\n",
        "    \"science fiction\": [\"science\", \"fiction\"],\n",
        "    \"action thriller\": [\"action\", \"thriller\"],\n",
        "    \"horror\": [\"horror\"],\n",
        "    \"superhero\": [\"superhero\"],\n",
        "    \"drama\": [\"drama\"],\n",
        "    \"detective\": [\"detective\"],\n",
        "    \"time travel\": [\"time\", \"travel\"],\n",
        "    \"political thriller\": [\"political\", \"thriller\"],\n",
        "    \"comedy\": [\"comedy\"],\n",
        "    \"adventure\": [\"adventure\"],\n",
        "    \"fantasy\": [\"fantasy\"],\n",
        "    \"mystery\": [\"mystery\"],\n",
        "    \"crime\": [\"crime\"]\n",
        "}\n",
        "\n",
        "results = boolean_evaluation(queries, df)\n",
        "\n",
        "for metric, values in results.items():\n",
        "    mean_value = np.mean(values) if values else 0\n",
        "    print(f\"Mean {metric}: {mean_value:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
